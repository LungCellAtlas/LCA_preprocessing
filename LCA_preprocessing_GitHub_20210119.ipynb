{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optional extension, for automatic pretty-formatting of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out warnings if wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import sys\n",
    "\n",
    "from math import isnan\n",
    "\n",
    "# plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# self-written modules:\n",
    "sys.path.append(\"./lca/\")\n",
    "import utils\n",
    "import LCA_file_reading # note that this script is not on GitHub yet, due to patient-specific information in script\n",
    "import preprocessing\n",
    "import ontology_based_harmonizing\n",
    "import scib_excerpts\n",
    "import dim_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### version info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print package versions (can do this manually too instead of using utils, if wanted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.print_loaded_module_versions(globals().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print module versions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE that the script LCA_file_reading is not made available on GitHub yet, since some of the data in there are patient-specific and cannot be published yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) adatas with one AnnData per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Reyfman/Misharin, VieiraBraga/Nawijn_nasal, VieiraBraga/Teichmann, Morse/Lafyatis, Misharin_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/path/to/dir/lung_cell_atlas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in each of the datasets:\n",
    "adatas_per_sample_dict = dict()\n",
    "# Reyfman et al.\n",
    "project_name_Reyfman = \"Northwestern_Misharin_2018Reyfman\"\n",
    "project_dir_Reyfman = data_dir + project_name_Reyfman + \"/\"\n",
    "print(\"\\n\" + project_name_Reyfman)\n",
    "adatas_per_sample_dict[\"Misharin\"] = LCA_file_reading.read_file_Reyfman(\n",
    "    project_dir=project_dir_Reyfman\n",
    ")\n",
    "# VieiraBraga/Nawijn nasal:\n",
    "project_name_VieiraBraga = \"Sanger_Teichmann_2019VieiraBraga\"\n",
    "project_dir_VieiraBraga = data_dir + project_name_VieiraBraga + \"/\"\n",
    "print(\"\\n\" + project_name_VieiraBraga, \"UMCG nasal\")\n",
    "adatas_per_sample_dict[\n",
    "    \"Nawijn_nasal\"\n",
    "] = LCA_file_reading.read_file_VieiraBraga_UMCG_nasal(\n",
    "    project_dir_VieiraBraga, donor_type=\"healthy\", verbose=True\n",
    ")\n",
    "# VieiraBraga/Sanger samples:\n",
    "print(\"\\n\" + project_name_VieiraBraga, \"Sanger parenchyma\")\n",
    "adatas_per_sample_dict[\"Teichmann\"] = LCA_file_reading.read_file_VieiraBraga_Sanger(\n",
    "    project_dir_VieiraBraga\n",
    ")\n",
    "# # Raredon et al\n",
    "# project_name_Raredon = \"Yale_Niklason_2019Raredon\"\n",
    "# project_dir_Raredon = data_dir + project_name_Raredon + \"/\"\n",
    "# print(\"\\n\" + project_name_Raredon)\n",
    "# adatas_per_sample_dict[\"Raredon\"] = LCA_file_reading.read_file_Raredon(\n",
    "#     project_dir=project_dir_Raredon\n",
    "# )\n",
    "# Morse et al\n",
    "project_name_Morse = \"Pittsburgh_Lafyatis_2019Morse\"\n",
    "project_dir_Morse = data_dir + project_name_Morse + \"/\"\n",
    "print(\"\\n\" + project_name_Morse)\n",
    "adatas_per_sample_dict[\"Lafyatis\"] = LCA_file_reading.read_file_Morse(\n",
    "    project_dir=project_dir_Morse\n",
    ")\n",
    "# Misharin new:\n",
    "project_name_Misharin_new = \"Misharin_new\"\n",
    "project_dir_Misharin_new = data_dir + project_name_Misharin_new + \"/\"\n",
    "adatas_per_sample_dict[\"Misharin_new\"] = LCA_file_reading.read_file_Misharin_new(\n",
    "    project_dir_Misharin_new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adatas_per_sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list number of samples per dict:\n",
    "for dataset_name, adatas in adatas_per_sample_dict.items():\n",
    "    print(dataset_name.upper() + \":\")\n",
    "    print(len(adatas.keys()), \"samples\")\n",
    "    total_cells = 0\n",
    "    for sample, adata in adatas.items():\n",
    "        n_cells = adata.shape[0]\n",
    "        total_cells = total_cells + n_cells\n",
    "    print(str(total_cells) + \" cells\\n\")\n",
    "    #     # check if each dataset has the required columns.\n",
    "    #     # take only first sample:\n",
    "    #     print(adatas[list(adatas.keys())[0]].obs.columns)\n",
    "#     print(adatas[list(adatas.keys())[0]].var.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) adatas with one AnnData per dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. Vieira-Braga/Nawijn_lung, Madissoon/Meyer, Habermann/Kropski, Travaglini/Krasnow, Deprez/Barbry, Goldfarbmuren/Seibold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas_per_dataset = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vieira-Braga/Nawijn lung:\n",
    "project_name_VieiraBraga = \"Sanger_Teichmann_2019VieiraBraga\"\n",
    "project_dir_VieiraBraga = data_dir + project_name_VieiraBraga + \"/\"\n",
    "print(project_name_VieiraBraga, \"UMCG lung\")\n",
    "adatas_per_dataset[\"Nawijn_lung\"] = LCA_file_reading.read_file_VieiraBraga_UMCG_lung(\n",
    "    project_dir=project_dir_VieiraBraga\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Madissoon et al\n",
    "project_name_Madissoon = \"Sanger_Meyer_2019Madissoon\"\n",
    "project_dir_Madissoon = data_dir + project_name_Madissoon + \"/\"\n",
    "print(project_name_Madissoon)\n",
    "adatas_per_dataset[\"Meyer\"] = LCA_file_reading.read_file_Madissoon(\n",
    "    project_dir=project_dir_Madissoon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habermann et al\n",
    "project_name_Habermann = \"Vanderbilt_Kropski_bioRxivHabermann\"\n",
    "project_dir_Habermann = data_dir + project_name_Habermann + \"/\"\n",
    "print(project_name_Habermann)\n",
    "adatas_per_dataset[\"Kropski\"] = LCA_file_reading.read_file_Habermann(\n",
    "    project_dir=project_dir_Habermann\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travaglini et al\n",
    "project_name_Travaglini = \"Stanford_Krasnow_bioRxivTravaglini\"\n",
    "project_dir_Travaglini = data_dir + project_name_Travaglini + \"/\"\n",
    "print(project_name_Travaglini)\n",
    "adatas_per_dataset[\"Krasnow\"] = LCA_file_reading.read_file_Travaglini(\n",
    "    project_dir=project_dir_Travaglini\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprez et al\n",
    "project_name_Deprez = \"CNRS_Barbry_bioRxivDeprez\"\n",
    "project_dir_Deprez = data_dir + project_name_Deprez + \"/\"\n",
    "print(project_name_Deprez)\n",
    "adatas_per_dataset[\"Barbry\"] = LCA_file_reading.read_file_Deprez(\n",
    "    project_dir=project_dir_Deprez\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goldfarbmuren et al\n",
    "project_name_Goldfarbmuren = \"NJH_Seibold_2020Goldfarbmuren\"\n",
    "project_dir_Goldfarbmuren = data_dir + project_name_Goldfarbmuren + \"/\"\n",
    "print(project_name_Goldfarbmuren)\n",
    "adatas_per_dataset[\"Seibold\"] = LCA_file_reading.read_file_Goldfarbmuren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, adata_object in adatas_per_dataset.items():\n",
    "    print(dataset_name, adata_object.obs.columns, adata_object.var.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add up duplicate genes and remove genes with 0 counts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first the per-sample AnnData dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, adatas in adatas_per_sample_dict.items():\n",
    "    print(dataset_name.toupper() + \":\\n\")\n",
    "    for sample_name, adata in adatas.items():\n",
    "        print(sample_name + \":\")\n",
    "        # filter out genes without counts\n",
    "        n_genes_before = adata.shape[1]\n",
    "        sc.pp.filter_genes(adata, min_counts=1)\n",
    "        # drop annotation label that is automatically created:\n",
    "        adata.var.drop(\"n_counts\", axis=1, inplace=True)\n",
    "        n_genes_after = adata.shape[1]\n",
    "        print(\"number of genes removed:\", n_genes_before - n_genes_after)\n",
    "        adatas[sample_name] = preprocessing.add_up_duplicate_gene_name_columns(adata)\n",
    "        print(adata.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now the per-dataset AnnData dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, adata in adatas_per_dataset.items():\n",
    "    print(dataset_name + \":\")\n",
    "    n_genes_before = adata.shape[1]\n",
    "    sc.pp.filter_genes(adata, min_counts=1)\n",
    "    # drop annoation column that is automatically created:\n",
    "    adata.var.drop(\"n_counts\", axis=1, inplace=True)\n",
    "    n_genes_after = adata.shape[1]\n",
    "    print(\"number of genes removed:\", n_genes_before - n_genes_after)\n",
    "    adatas_per_dataset[dataset_name] = preprocessing.add_up_duplicate_gene_name_columns(\n",
    "        adata\n",
    "    )\n",
    "    print(str(adata.shape) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pool per-sample AnnData objects to per-dataset AnnData objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name, adatas in adatas_per_sample_dict.items():\n",
    "    print(\"Dataset: \" + dataset_name)\n",
    "    adata = sc.AnnData.concatenate(\n",
    "        *adatas.values(),\n",
    "        join=\"outer\",\n",
    "        batch_key=\"sample_temp\",\n",
    "        batch_categories=list(adatas.keys()),\n",
    "        index_unique=\"_\"\n",
    "    )\n",
    "    # remove sample name column:\n",
    "    adata.obs.drop(\"sample_temp\", axis=1, inplace=True)\n",
    "    # set nan to zero\n",
    "    adata.X = np.nan_to_num(adata.X)\n",
    "    print(adata.shape)\n",
    "    # shuffle rows:\n",
    "    index_list = np.arange(adata.shape[0])\n",
    "    np.random.shuffle(index_list)\n",
    "    adata = adata[index_list]\n",
    "    adatas_per_dataset[dataset_name] = adata.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if datasets have GRCh37 or GRCh38 gene names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_37 = [\"AC000032.2\", \"AC000036.4\", \"ZNF724P\", \"ZNF812\", \"AC000068.10\"]\n",
    "genes_38 = [\"AC000032.1\", \"AC000036.1\", \"ZNF724\", \"ZNF812P\", \"AC000068.1\"]\n",
    "for gene_37, gene_38 in zip(genes_37, genes_38):\n",
    "    print(gene_37)\n",
    "    for dataset in adatas_per_dataset.keys():\n",
    "        found_gene = \"\"\n",
    "        if gene_37 in adatas_per_dataset[dataset].var.index:\n",
    "            found_gene = found_gene + \"37\"\n",
    "        if gene_38 in adatas_per_dataset[dataset].var.index:\n",
    "            found_gene = found_gene + \"38\"\n",
    "        print(dataset, found_gene)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pool between datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData.concatenate(\n",
    "    *adatas_per_dataset.values(),\n",
    "    join=\"outer\",\n",
    "    batch_key=\"dataset_temp\",\n",
    "    batch_categories=list(adatas_per_dataset.keys()),\n",
    "    index_unique=\"_\"\n",
    ")\n",
    "print(adata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove columns that were automatically added when concatenating AnnDatas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.drop([\"dataset_temp\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double check if all datasets had ensembl84 genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_84 = [\"AC000032.2\", \"AC000036.4\", \"ZNF724P\", \"AC000068.10\"]\n",
    "genes_93 = [\"AC000032.1\", \"AC000036.1\", \"ZNF724\", \"AC000068.1\"]\n",
    "for gene_84, gene_93 in zip(genes_84, genes_93):\n",
    "    print(gene_84)\n",
    "    found_gene = \"\"\n",
    "    if gene_84 in adata.var.index:\n",
    "        found_gene = found_gene + \"84\"\n",
    "    if gene_93 in adata.var.index:\n",
    "        found_gene = found_gene + \"93\"\n",
    "    print(found_gene)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = np.nan_to_num(adata.X)\n",
    "index_list = np.arange(adata.shape[0])\n",
    "np.random.shuffle(index_list)\n",
    "adata = adata[index_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct abnormal gene names, and remove duplicate gene names\n",
    "(also for compatibility with R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_backup = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct gene names\n",
    "renamer_dict = preprocessing.get_gene_renamer_dict(adata.var.index.tolist().copy())\n",
    "n_genes_to_rename = np.sum(old != new for old, new in renamer_dict.items())\n",
    "print(n_genes_to_rename, \"GENES TO BE RENAMED (old, new):\\n\")\n",
    "for old, new in renamer_dict.items():\n",
    "    if len(new) > 0:\n",
    "        print(old, new)\n",
    "adata.var[\"original_gene_names\"] = adata.var.index.tolist().copy()\n",
    "translation_dict = dict(zip(adata.var.index, adata.var.index))\n",
    "for gene_to_rename, new_name in renamer_dict.items():\n",
    "    translation_dict[gene_to_rename] = new_name\n",
    "adata.var.index = adata.var.index.map(translation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any new duplicates that might have emerged:\n",
    "adata = preprocessing.add_up_duplicate_gene_name_columns(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cell annotations related to QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add cell annotations such as total counts per cell, percentage of mitochondrial reads etc. This has to be done before any filtering or normalizing is performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate with QC stuff:\n",
    "adata = preprocessing.add_cell_annotations(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add cell ontology annotation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original cell type labeling (as provided by dataset providers) will now be translated to the current version of the cell type ontology, consisting of 5 levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "correct/check some dataset and cluster naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct two incomplete cluster names Misharin dataset:\n",
    "cells_to_change = [\n",
    "    cell\n",
    "    for cell, ann in zip(adata.obs.index, adata.obs.original_celltype_ann)\n",
    "    if ann in [\"C7_2_DC2_FCER1A\", \"C7_7_DC1_CLEC9A\"]\n",
    "]\n",
    "if len(cells_to_change) > 0:\n",
    "    print(\"adapting cell type annotations of wrongly named types from Sasha\")\n",
    "    # convert to list (not necessary for function)\n",
    "    adata.obs.original_celltype_ann = list(adata.obs.original_celltype_ann.values)\n",
    "    adata.obs.loc[cells_to_change, \"original_celltype_ann\"] = [\n",
    "        sample + \"_\" + label\n",
    "        for sample, label in zip(\n",
    "            adata.obs.loc[cells_to_change, \"sample\"],\n",
    "            adata.obs.loc[cells_to_change, \"original_celltype_ann\"],\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure data nawijn is split to two, since they have different original annotations (see cell type ontology)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.dataset = list(adata.obs.dataset)\n",
    "nawijn_lung_cells = adata.obs.apply(\n",
    "    lambda x: x[\"lung_vs_nasal\"] == \"lung\"\n",
    "    and x[\"dataset\"] == \"UMCG_Nawijn_2019VieiraBraga\",\n",
    "    axis=1,\n",
    ")\n",
    "print(\"number of Nawijn lung cells with only general dataset annotation:\", np.sum(nawijn_lung_cells))\n",
    "nawijn_nasal_cells = adata.obs.apply(\n",
    "    lambda x: x[\"lung_vs_nasal\"] == \"nasal\"\n",
    "    and x[\"dataset\"] == \"UMCG_Nawijn_2019VieiraBraga\",\n",
    "    axis=1,\n",
    ")\n",
    "print(\"number of Nawijn nasal cells with only general dataset annotation:\", np.sum(nawijn_lung_cells))\n",
    "\n",
    "\n",
    "adata.obs.loc[nawijn_lung_cells, \"dataset\"] = \"UMCG_Nawijn_2019VieiraBraga_lung\"\n",
    "adata.obs.loc[nawijn_nasal_cells, \"dataset\"] = \"UMCG_Nawijn_2019VieiraBraga_nasal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the .csv that contains the translations:  \n",
    "NOTE that this file is not made publicly available yet, but will be made available with publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonizing_df = ontology_based_harmonizing.load_harmonizing_table(\n",
    "    \"/path/to/dir/LCA/ontologies/cell_type_ontologies/cell_type_ontology_20201012.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe that contains each cell type name from the consensus ontology as indices, with their matching annotations at the other levels. This will simplify mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_df = ontology_based_harmonizing.create_consensus_table(harmonizing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dataframe that for each original celltype annotation (from all datasets pooled) provides the translation to the consensus ontology at all levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_translation_df = ontology_based_harmonizing.create_celltype_to_consensus_translation_df(\n",
    "    adata, consensus_df, harmonizing_df, verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now translate the original annotations to the consensus in your AnnData:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ontology_based_harmonizing.consensus_annotate_anndata(\n",
    "    adata, celltype_translation_df, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add sample/donor annotations from LCA metadata tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this part of the script cannot be run yet, since you'll need a file with patient information that cannot be made public yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the naming of the _samples_ is harmonized rather than the donor naming, so use sample names to copy metadata to AnnData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = preprocessing.get_sample_annotation_table_LCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that [patient specific information, removed for GitHub version of script for now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_in_adata = sorted(set(adata.obs[\"sample\"]))\n",
    "samples_in_metadata = metadata.index\n",
    "print(\"n samples in adata:\", len(samples_in_adata))\n",
    "if len(samples_in_metadata) != len(set(samples_in_metadata)):\n",
    "    print(\"WARNING: DUPLICATE SAMPLE NAMES IN METADATA TABLE! THIS SHOULD BE FIXED.\")\n",
    "for sample in samples_in_adata:\n",
    "    if sample not in samples_in_metadata:\n",
    "        print(sample, \"is in AnnData object but not in metadata. Check this.\")\n",
    "for sample in samples_in_metadata:\n",
    "    if sample not in samples_in_adata:\n",
    "        print(sample, \"is in metadata but not in AnnData object. Check this.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns_to_drop = [\"IF AVAILABLE/ APPLICABLE -->\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.drop(columns=metadata_columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in metadata.columns:\n",
    "    sample_to_cat_dict = dict(zip(metadata.index, metadata[cat]))\n",
    "    adata.obs[cat] = adata.obs[\"sample\"].map(sample_to_cat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check within-dataset diversity of technical covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.groupby(\"dataset\").agg(\n",
    "    {\n",
    "        \"cell ranger version \": \"nunique\",\n",
    "        \"disease status\": \"nunique\",\n",
    "        \"fresh or frozen\": \"nunique\",\n",
    "        \"known lung disease\": \"nunique\",\n",
    "        \"sample type\": \"nunique\",\n",
    "        \"sequencing platform\": \"nunique\",\n",
    "        \"single cell platform\": \"nunique\",\n",
    "        \"subject type\": \"nunique\",\n",
    "        \"tissue dissociation protocol\": \"nunique\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting of datasets into separate batches, where necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three datasets should be split into seperate batches:  \n",
    "    - kropski/banovich: they come from two different institutes, can be derived from donor names  \n",
    "    - lafyatis: includes both 10xv1 and 10xv2  \n",
    "    - seibold: includes both 10xv2 and 10xv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(adata.obs.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_dataset_df = adata.obs.groupby(\"sample\").agg(\n",
    "    {\"sample\": \"first\", \"dataset\": \"first\", \"single cell platform\": \"first\"}\n",
    ")\n",
    "sample_to_dataset_dict = dict(\n",
    "    zip(sample_to_dataset_df[\"sample\"], sample_to_dataset_df.dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kropski/banovich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dna = {\n",
    "    sample: \"Vanderbilt_Kropski_bioRxivHabermann_dna\"\n",
    "    for sample in [\"F01157\", \"F01174\", \"F01365\", \"F01366\", \"F01367\"]\n",
    "}\n",
    "samples_vand = {\n",
    "    sample: \"Vanderbilt_Kropski_bioRxivHabermann_vand\"\n",
    "    for sample in [\"HD65\", \"HD66\", \"HD67\", \"HD68\", \"F00409\", \"HD70\", \"F01394\"]\n",
    "}\n",
    "sample_to_dataset_dict.update(samples_dna)\n",
    "sample_to_dataset_dict.update(samples_vand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lafyatis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_lafy_v1 = {\n",
    "    sample: \"Pittsburgh_Lafyatis_2019Morse_10Xv1\"\n",
    "    for sample in sample_to_dataset_df.index\n",
    "    if sample_to_dataset_df.loc[sample, \"dataset\"] == \"Pittsburgh_Lafyatis_2019Morse\"\n",
    "    and sample_to_dataset_df.loc[sample, \"single cell platform\"] == \"10x_3'_v1\"\n",
    "}\n",
    "samples_lafy_v2 = {\n",
    "    sample: \"Pittsburgh_Lafyatis_2019Morse_10Xv2\"\n",
    "    for sample in sample_to_dataset_df.index\n",
    "    if sample_to_dataset_df.loc[sample, \"dataset\"] == \"Pittsburgh_Lafyatis_2019Morse\"\n",
    "    and sample_to_dataset_df.loc[sample, \"single cell platform\"] == \"10x_3'_v2\"\n",
    "}\n",
    "sample_to_dataset_dict.update(samples_lafy_v1)\n",
    "sample_to_dataset_dict.update(samples_lafy_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seibold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_seibold_v2 = {\n",
    "    sample: \"NJH_Seibold_2020Goldfarbmuren_10Xv2\"\n",
    "    for sample in sample_to_dataset_df.index\n",
    "    if sample_to_dataset_df.loc[sample, \"dataset\"] == \"NJH_Seibold_2020Goldfarbmuren\"\n",
    "    and sample_to_dataset_df.loc[sample, \"single cell platform\"] == \"10x_3'_v2\"\n",
    "}\n",
    "samples_seibold_v3 = {\n",
    "    sample: \"NJH_Seibold_2020Goldfarbmuren_10Xv3\"\n",
    "    for sample in sample_to_dataset_df.index\n",
    "    if sample_to_dataset_df.loc[sample, \"dataset\"] == \"NJH_Seibold_2020Goldfarbmuren\"\n",
    "    and sample_to_dataset_df.loc[sample, \"single cell platform\"] == \"10x_3'_v3\"\n",
    "}\n",
    "sample_to_dataset_dict.update(samples_seibold_v2)\n",
    "sample_to_dataset_dict.update(samples_seibold_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"dataset\"] = adata.obs[\"sample\"].map(sample_to_dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge Nawijn dataset to one (for downstream analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = sorted(set(adata.obs.dataset))\n",
    "# convert Martijn's datasets to just one dataset:\n",
    "dataset_to_dataset_mapping = dict(zip(datasets, datasets))\n",
    "dataset_to_dataset_mapping[\n",
    "    \"UMCG_Nawijn_2019VieiraBraga_lung\"\n",
    "] = \"UMCG_Nawijn_2019VieiraBraga\"\n",
    "dataset_to_dataset_mapping[\n",
    "    \"UMCG_Nawijn_2019VieiraBraga_nasal\"\n",
    "] = \"UMCG_Nawijn_2019VieiraBraga\"\n",
    "adata.obs.dataset = adata.obs.dataset.map(dataset_to_dataset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set(adata.obs.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove diseased subject(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out asthma patients:\n",
    "# T84: seibold childhood asthma\n",
    "# THD0002 and VUHD65: asthma from Kropski/Banovich\n",
    "asthma_subjects = [\"T84\", \"THD0002\", \"VUHD65\"]\n",
    "n_cells_before = adata.shape[0]\n",
    "filter = [subj not in asthma_subjects for subj in adata.obs.subject_ID]\n",
    "if sum(filter) < adata.shape[0]:\n",
    "    adata = adata[filter, :].copy()\n",
    "n_cells_after = adata.shape[0]\n",
    "print(\n",
    "    \"number of cells removed by filtering out asthmatic subjects:\",\n",
    "    n_cells_before - n_cells_after,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata cleanup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To correct (part of this might have already been fixed with corrected reading scripts, check that next time I run this notebook):  \n",
    "- missing lung_vs_nasal annotation for Seibold data (fixed in read_file script)\n",
    "- barbry column is \"enrichment\", all others are \"enrichment \"  \n",
    "- same for cellranger version: harmonize. Also, 3.2.0 should be 3.0.2 for Barbry\n",
    "- barbry data includes 'H', 'F' and 'M' as sex. translate to male/female\n",
    "- misharin data misses 3' info\n",
    "- missing institute info for kropski/banovich data\n",
    "- cell viability should be numerical: remove trailing \"%\" and change to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seibold_cells = adata[adata.obs[\"last_author/PI\"] == \"Seibold\", :].obs.index\n",
    "barbry_cells = adata[adata.obs[\"last_author/PI\"] == \"Barbry/Leroy\", :].obs.index\n",
    "misharin_cells = adata.obs.loc[\n",
    "    [x in [\"Misharin\", \"Misharin/Budinger\"] for x in adata.obs[\"last_author/PI\"]], :\n",
    "].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seibold data to lung\n",
    "adata.obs.loc[seibold_cells, \"lung_vs_nasal\"] = \"lung\"\n",
    "# fix barbry column misnaming, and mistake in cell ranger version:\n",
    "if \"enrichment\" in adata.obs.columns:\n",
    "    print(\"adapting enrichment and column\")\n",
    "    adata.obs[\"enrichment \"] = adata.obs[\"enrichment \"].tolist()\n",
    "    adata.obs.loc[barbry_cells, \"enrichment \"] = \"No\"\n",
    "    adata.obs.drop(columns=[\"enrichment\"], inplace=True)\n",
    "if \"cell ranger version\" in adata.obs.columns:\n",
    "    print(\"adapting cell ranger version and column\")\n",
    "    adata.obs.loc[barbry_cells, \"cell ranger version \"] = \"3.0.2\"\n",
    "    adata.obs.drop(columns=[\"cell ranger version\"], inplace=True)\n",
    "if list(set(adata.obs.loc[barbry_cells, :][\"cell ranger version \"])) != [\"3.0.2\"]:\n",
    "    print(\"adapting cell ranger version\")\n",
    "    adata.obs.loc[barbry_cells, \"cell ranger version \"] = \"3.0.2\"\n",
    "# translate sex annotations from Barbry to common terminology\n",
    "sex_dict = {\"female\":\"female\",\"male\":\"male\", \"F\":\"female\",\"H\":\"male\",\"M\":\"male\"}\n",
    "adata.obs.sex = adata.obs.sex.map(sex_dict)\n",
    "# set Misharin data to 3' (missing annotation)\n",
    "adata.obs.loc[misharin_cells, \"3' or 5'\"] = \"3'\"\n",
    "# generate institute annotation\n",
    "dataset_to_institute_df = adata.obs.groupby(\"dataset\").agg(\n",
    "    {\"dataset\": \"first\", \"Institute\": \"first\"}\n",
    ")\n",
    "dataset_to_institute_dict = dict(\n",
    "    zip(dataset_to_institute_df.dataset, dataset_to_institute_df[\"Institute\"])\n",
    ")\n",
    "dataset_to_institute_dict[\n",
    "    \"Vanderbilt_Kropski_bioRxivHabermann_dna\"\n",
    "] = \"Donor Network of Arizona\"\n",
    "dataset_to_institute_dict[\"Vanderbilt_Kropski_bioRxivHabermann_vand\"] = \"Vanderbilt\"\n",
    "adata.obs[\"Institute\"] = adata.obs[\"dataset\"].map(dataset_to_institute_dict)\n",
    "# convert viability percentage annotation to float instead of string\n",
    "def conv_string_perc_to_float(string_percentage):\n",
    "    stripped_string = string_percentage.rstrip(\" \")\n",
    "    stripped_string = stripped_string.rstrip(\"%\")\n",
    "    return float(stripped_string)\n",
    "\n",
    "\n",
    "conv_string_perc_to_float = np.vectorize(conv_string_perc_to_float)\n",
    "adata.obs[\"cell viability %\"] = conv_string_perc_to_float(b\n",
    "    adata.obs[\"cell viability %\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run metadata_cleaner function to further harmonize metadata annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs = preprocessing.metadata_cleaner(adata.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonize anatomical region:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first add prefix to annotations from Barbry data, since their naming is inconsistent with other dataset's naming. Not adding prefix will result in mix-ups of translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix barbry detailed annotation with coarse:\n",
    "adata.obs[\"anatomical region detailed\"] = adata.obs[\n",
    "    \"anatomical region detailed\"\n",
    "].tolist()\n",
    "adata_barbry = adata[adata.obs[\"last_author/PI\"] == \"Barbry/Leroy\", :].copy()\n",
    "barbry_region_detailed_prefixed = [\n",
    "    x + \"_\" + y\n",
    "    for x, y in zip(\n",
    "        adata_barbry.obs[\"anatomical region coarse\"],\n",
    "        adata_barbry.obs[\"anatomical region detailed\"],\n",
    "    )\n",
    "]\n",
    "adata.obs.loc[\n",
    "    adata_barbry.obs.index, \"anatomical region detailed\"\n",
    "] = barbry_region_detailed_prefixed\n",
    "del adata_barbry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now harmonize anatomical region:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in harmonizing table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this part of the script cannot be run yet, since the matching files cannot be made public yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonizing_df = ontology_based_harmonizing.load_harmonizing_table(\n",
    "    \"/path/to/dir/LCA/ontologies/anatomical_region_ontologies/LCA_ontologies_anatomical_region_ontology_20201120.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create translation table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_df = ontology_based_harmonizing.create_consensus_table(\n",
    "    harmonizing_df, max_level=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate both levels (coarse and fine) to their harmonized counterpart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in [\"coarse\", \"fine\"]:\n",
    "    translation_df = (\n",
    "        ontology_based_harmonizing.create_celltype_to_consensus_translation_df(\n",
    "            adata,\n",
    "            consensus_df,\n",
    "            harmonizing_df,\n",
    "            verbose=False,\n",
    "            ontology_type=\"anatomical_region_\" + res,\n",
    "        )\n",
    "    )\n",
    "    adata = ontology_based_harmonizing.consensus_annotate_anndata(\n",
    "        adata,\n",
    "        translation_df,\n",
    "        verbose=True,\n",
    "        max_ann_level=3,\n",
    "        ontology_type=\"anatomical_region_\" + res,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge coarse and fine annotations, so that we keep the finest annotation available for every sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ontology_based_harmonizing.merge_coarse_and_fine_anatomical_ontology_anns(\n",
    "    adata, remove_harm_coarse_and_fine_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### harmonize nan/None/\"nan\" etc, clean metadata with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all different types of None/NaN to np.nan\n",
    "none_entries = adata.obs.applymap(utils.check_if_nan)\n",
    "adata.obs = adata.obs.mask(none_entries.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now clean up metadata again? (Is that necessary?)\n",
    "adata.obs = preprocessing.metadata_cleaner(adata.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check that BMI is numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.groupby([\"dataset\", \"subject_ID\"]).agg({\"BMI\": \"first\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"BMI\"].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if dtype is not float, check the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(adata.obs[\"BMI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"BMI\"] = adata.obs[\"BMI\"].values.astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add age annotation (merging of age in years and age range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add age as merger of 'age, in years' and 'age, range'\n",
    "adata.obs[\"age\"] = [\n",
    "    preprocessing.age_converter(age, age_range)\n",
    "    for age, age_range in zip(adata.obs[\"age, in years\"], adata.obs[\"age, range\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMS pseudonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS IS CUT FROM THE SCRIPT, SINCE THE PSEUDONYMIZATION CANNOT BE MADE PUBLIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"study\" annotation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study is a simplified version of dataset, in which studies are not split based on e.g. sequencing platform version (see e.g. Lafyatis datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_study_mapper = {\n",
    "    \"Sanger_Meyer_2019Madissoon\": \"Meyer_2019\",\n",
    "    \"Vanderbilt_Kropski_bioRxivHabermann_vand\": \"Kropski_2020\",  # _vand\",\n",
    "    \"CNRS_Barbry_bioRxivDeprez\": \"Barbry_2020\",\n",
    "    \"Pittsburgh_Lafyatis_2019Morse_10Xv2\": \"Lafyatis_2019\",  # _10Xv2\",\n",
    "    \"NJH_Seibold_2020Goldfarbmuren_10Xv3\": \"Seibold_2020\",  # _10Xv3\",\n",
    "    \"Stanford_Krasnow_bioRxivTravaglini\": \"Krasnow_2020\",\n",
    "    \"Misharin_new\": \"Misharin_unpubl\",\n",
    "    \"Sanger_Teichmann_2019VieiraBraga\": \"Teichmann_2019\",\n",
    "    \"UMCG_Nawijn_2019VieiraBraga\": \"Nawijn_2019\",\n",
    "    \"NJH_Seibold_2020Goldfarbmuren_10Xv2\": \"Seibold_2020\",  # _10Xv2\",\n",
    "    \"Northwestern_Misharin_2018Reyfman\": \"Misharin_2018\",\n",
    "    \"Vanderbilt_Kropski_bioRxivHabermann_dna\": \"Kropski_2020\",  # _dna\",\n",
    "    \"Pittsburgh_Lafyatis_2019Morse_10Xv1\": \"Lafyatis_2019\",  # _10Xv1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['study'] = adata.obs.dataset.map(dataset_to_study_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if number of unique annotations per dataset, per category are as expected (these should be mostly \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.groupby(\"dataset\").agg(\n",
    "    {\n",
    "        \"cell ranger version \": \"nunique\",\n",
    "        \"disease status\": \"nunique\",\n",
    "        \"fresh or frozen\": \"nunique\",\n",
    "        \"known lung disease\": \"nunique\",\n",
    "        \"sample type\": \"nunique\",\n",
    "        \"sequencing platform\": \"nunique\",\n",
    "        \"single cell platform\": \"nunique\",\n",
    "        \"subject type\": \"nunique\",\n",
    "        \"tissue dissociation protocol\": \"nunique\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check which datasets are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(adata.obs.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(adata.obs.study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(adata.obs[\"last_author/PI\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if all variables have values for all cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we only expect that for some of them, but good to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in adata.obs.columns:\n",
    "    if isinstance(adata.obs[cat].values, np.ndarray):\n",
    "        print(cat, np.nan in adata.obs[cat].values, \"nan\" in adata.obs[cat].values)\n",
    "    else:\n",
    "        print(cat, adata.obs[cat].values.isna().any(), \"nan\" in adata.obs[cat].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if donor and sample names occur in only one dataset each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = adata.obs.groupby(\"sample\").agg({\"dataset\": \"nunique\"})\n",
    "# check if sample names only occur in one dataset:\n",
    "for sample in temp.index:\n",
    "    if temp.loc[sample, \"dataset\"] != 1:\n",
    "        print(str(sample) + \": this sample name occurs in multiple datasets\")\n",
    "temp = adata.obs.groupby(\"subject_ID\").agg({\"dataset\": \"nunique\"})\n",
    "for donor in temp.index:\n",
    "    if temp.loc[donor, \"dataset\"] != 1:\n",
    "        print(\n",
    "            str(donor)\n",
    "            + \": this subject_ID name occurs in \"\n",
    "            + str(temp.loc[donor, \"dataset\"])\n",
    "            + \" datasets!\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if all values have only zeros as decimals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store remainders of division by 1, count for each row number of entries for which remainder is not 0 (they should all be zero if data are integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.sum(adata.X.toarray() % 1 == 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select only those rows of adata that have non-integer values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonint_adata = adata[test != adata.shape[1], :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check shape, it should have zero rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonint_adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if it doesn't have zero rows, then check which datasets have non-integer values (in that case we received non-raw counts from them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(nonint_adata.obs.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write/read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.write(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_RAW_annotated.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = sc.read(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_RAW_annotated.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter out cells with low total counts and genes expressed in low number of cells:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out all cells with fewer than 500 cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells_pre = adata.shape[0]\n",
    "sc.pp.filter_cells(adata, min_counts=500)\n",
    "n_cells_post = adata.shape[0]\n",
    "print(\"Number of cells removed: \" + str(n_cells_pre - n_cells_post))\n",
    "print(\"Number of cells pre-filtering: \" + str(n_cells_pre))\n",
    "print(\"Number of cells post filtering: \" + str(n_cells_post))\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove genes that are expressed in fewer than 5 cells after cell filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes_pre = adata.shape[1]\n",
    "sc.pp.filter_genes(adata, min_cells=5)\n",
    "n_genes_post = adata.shape[1]\n",
    "print(\"Number of genes removed: \" + str(n_genes_pre - n_genes_post))\n",
    "print(\"Number of genes pre-filtering: \" + str(n_genes_pre))\n",
    "print(\"Number of genes post filtering: \" + str(n_genes_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize and log-transform with SCRAN normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize with scran, we need to cluster first, so we'll go through standard (non-sophisticated) normalization for a temporary adata, then calculate the size-factors on there, and then use those size factors to normalize the original anndata. All of this is done in the SCRAN_normalize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(adata.obs[\"log10_total_counts\"], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# started at 10:30 ran till 12:41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_norm = scib_excerpts.SCRAN_normalize(adata, log_transform=False) # started at "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if counts layer has integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(adata_norm.layers['counts'][:10,:], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if new adata.X has non-integers (it should):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(adata_norm.X[:10,:], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for random row and column if the size factor and original count lead to the corrected count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "j = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers['counts'][i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs.size_factors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X[4,9] * adata.obs.size_factors[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write/read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_norm.write(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_SCRAN_normalized.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = sc.read(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_SCRAN_normalized.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HIGHLY VARIABLE GENE SELECTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select highly variable genes. We will first calculate highly variable genes per sample, then look for the genes that are highly variable in all samples and select those, all samples but one, all samples but two, etc. until we have 2000 overall highly variable genes. Code from the scib_excerpts module is taken from https://github.com/theislab/scib. See also https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_nonlog = adata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvgs = scib_excerpts.hvg_batch(adata, batch_key=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check selected hvgs, see if they make sense:\n",
    "means = np.mean(adata_nonlog.X, axis=0)\n",
    "variances = np.var(adata_nonlog.X.toarray(), axis=0)\n",
    "dispersions = variances / means\n",
    "boolean_to_color = {\n",
    "    True: \"crimson\",\n",
    "    False: \"steelblue\",\n",
    "}  # make a dictionary that translates the boolean to colors\n",
    "hvg_colors = [\n",
    "    boolean_to_color[x in hvgs] for x in adata_nonlog.var.index\n",
    "]  # 'convert' the boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    np.log1p(means).tolist()[0], np.log(dispersions).tolist()[0], s=2, c=hvg_colors\n",
    ")\n",
    "plt.xlabel(\"log1p(mean)\")\n",
    "plt.ylabel(\"log(dispersion)\")\n",
    "plt.title(\"DISPERSION VERSUS MEAN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[\"highly_variable\"] = [x in hvgs for x in adata.var.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata_nonlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read/write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata.write(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barbry_Krasnow_Kropski_Lafyatis_Meyer_Misharin_MisharinNew_Nawijn_Teichmann_SCRAN_normalized_HVGann.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = sc.read(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barbry_Krasnow_Kropski_Lafyatis_Meyer_Misharin_MisharinNew_Nawijn_Teichmann_SCRAN_normalized_HVGann.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log-transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform log-transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform principal component analysis for dimensionality reduction, to reduce noise, and the limit computation time in downstream steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.tl.pca(adata, n_comps=200, copy=True, use_highly_variable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.pca_variance_ratio(adata, n_pcs=200, show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at variance explained per principal component. At least all PCs that precede the 'elbow' in the plot should be included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variance explained plots:\n",
    "dim_reduction.PCA_var_explained_plots(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll take ... PCs\n",
    "adata = sc.tl.pca(adata, n_comps=50, copy=True, use_highly_variable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check which genes have highest loadings for the first 10 PCs. This can be a useful first glance at which genes contribute most the the structure of your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.pca_loadings(adata, components=range(1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now run UMAP for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, n_neighbors=50)\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot QC factors and annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfig = sc.pl.umap(\n",
    "    adata,\n",
    "    color=adata.obs.columns.tolist(),\n",
    "    sort_order=False,\n",
    "    hspace=0.5,\n",
    "    #     wspace=0.65,\n",
    "    ncols=2,\n",
    "    return_fig=True,\n",
    "    #     save=\"/path/to/dir/test.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfig.savefig(\n",
    "    \"/path/to/dir/Barbry_Krasnow_Kropski_Lafyatis_Meyer_Misharin_MisharinNew_Nawijn_Teichmann_log1p_umaps.png\",\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    adata, color=[\"dataset\", \"ann_level_4\", \"ann_level_5\",], ncols=1, hspace=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(\n",
    "    adata, color=[\"sample\",], ncols=1, wspace=0.40,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read/write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store result:\n",
    "# adata.write(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_log1p.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata = sc.read(\n",
    "#     \"/path/to/dir/LCA_h5ads/Barb_Kras_Krop_Lafy_Meye_Mish_MishNew_Nawi_Seib_Teic_log1p.h5ad\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
